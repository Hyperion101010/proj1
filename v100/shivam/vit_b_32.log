Use GPU: 0 for training
=> creating model 'vit_b_32'
=> Dummy data is used!
class_token torch.Size([1, 1, 768])
conv_proj.weight torch.Size([768, 3, 32, 32])
conv_proj.bias torch.Size([768])
encoder.pos_embedding torch.Size([1, 50, 768])
encoder.layers.encoder_layer_0.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_0.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_0.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_0.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_0.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_0.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_0.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_0.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_0.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_0.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_0.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_0.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_1.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_1.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_1.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_1.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_1.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_1.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_1.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_1.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_1.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_1.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_1.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_1.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_2.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_2.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_2.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_2.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_2.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_2.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_2.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_2.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_2.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_2.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_2.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_2.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_3.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_3.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_3.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_3.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_3.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_3.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_3.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_3.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_3.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_3.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_3.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_3.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_4.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_4.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_4.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_4.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_4.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_4.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_4.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_4.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_4.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_4.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_4.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_4.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_5.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_5.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_5.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_5.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_5.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_5.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_5.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_5.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_5.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_5.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_5.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_5.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_6.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_6.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_6.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_6.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_6.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_6.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_6.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_6.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_6.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_6.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_6.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_6.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_7.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_7.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_7.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_7.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_7.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_7.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_7.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_7.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_7.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_7.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_7.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_7.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_8.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_8.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_8.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_8.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_8.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_8.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_8.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_8.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_8.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_8.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_8.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_8.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_9.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_9.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_9.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_9.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_9.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_9.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_9.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_9.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_9.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_9.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_9.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_9.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_10.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_10.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_10.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_10.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_10.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_10.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_10.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_10.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_10.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_10.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_10.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_10.mlp.3.bias torch.Size([768])
encoder.layers.encoder_layer_11.ln_1.weight torch.Size([768])
encoder.layers.encoder_layer_11.ln_1.bias torch.Size([768])
encoder.layers.encoder_layer_11.self_attention.in_proj_weight torch.Size([2304, 768])
encoder.layers.encoder_layer_11.self_attention.in_proj_bias torch.Size([2304])
encoder.layers.encoder_layer_11.self_attention.out_proj.weight torch.Size([768, 768])
encoder.layers.encoder_layer_11.self_attention.out_proj.bias torch.Size([768])
encoder.layers.encoder_layer_11.ln_2.weight torch.Size([768])
encoder.layers.encoder_layer_11.ln_2.bias torch.Size([768])
encoder.layers.encoder_layer_11.mlp.0.weight torch.Size([3072, 768])
encoder.layers.encoder_layer_11.mlp.0.bias torch.Size([3072])
encoder.layers.encoder_layer_11.mlp.3.weight torch.Size([768, 3072])
encoder.layers.encoder_layer_11.mlp.3.bias torch.Size([768])
encoder.ln.weight torch.Size([768])
encoder.ln.bias torch.Size([768])
heads.head.weight torch.Size([1000, 768])
heads.head.bias torch.Size([1000])
VE: model VisionTransformer(
  (conv_proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))
  (encoder): Encoder(
    (dropout): Dropout(p=0.0, inplace=False)
    (layers): Sequential(
      (encoder_layer_0): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_1): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_2): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_3): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_4): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_5): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_6): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_7): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_8): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_9): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_10): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
      (encoder_layer_11): EncoderBlock(
        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): MLPBlock(
          (0): Linear(in_features=768, out_features=3072, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=3072, out_features=768, bias=True)
          (4): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  (heads): Sequential(
    (head): Linear(in_features=768, out_features=1000, bias=True)
  )
)
Session Started at t: 2025-04-17 13:46:00.404541
Memory at Training Start (GPU): 336.55 MB
Reserved Memory at Start (GPU): 382.00 MB
conv_proj: <class 'torch.nn.modules.conv.Conv2d'> : torch.Size([1, 3, 224, 224]) : torch.Size([1, 768, 7, 7])
encoder.dropout: <class 'torch.nn.modules.dropout.Dropout'> : torch.Size([1, 50, 768]) : torch.Size([1, 50, 768])
encoder.layers.encoder_layer_0.ln_1: <class 'torch.nn.modules.normalization.LayerNorm'> : torch.Size([1, 50, 768]) : torch.Size([1, 50, 768])
